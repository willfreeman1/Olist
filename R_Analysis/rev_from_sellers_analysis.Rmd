```{r}

setwd("C://Users//willf//OneDrive//Documents//NYDSA//R//Olist//Data")

# Looking at various dataframe sizes to understand what I can use

consolidated_orders_size <- object.size(consolidated_orders)/(1024^2)
consolidated_orders_size



```

```{r}
# Create Olist Rev from Sellers DF

# First merge orders and order_items

library(dplyr)
library(tidyr)

orders_items = merge(orders, order_items, by = "order_id")

# Select necessary columns
rev_from_sellers <- orders_items %>%
  select(order_id, order_status, order_purchase_timestamp, order_item_id, product_id, seller_id, price, freight_value)


# Create new vs old sellers
rev_from_sellers <- rev_from_sellers %>%
  mutate(order_purchase_timestamp = as.POSIXct(order_purchase_timestamp, format = "%Y-%m-%d %H:%M:%S")) %>%
  group_by(seller_id) %>%
  arrange(order_purchase_timestamp) %>%
  mutate(
    seller_status = ifelse(order_purchase_timestamp == min(order_purchase_timestamp), "new", "old")
  ) %>%
  ungroup()

# Fix odd seller id with seller status marked NA for some reason
rev_from_sellers <- rev_from_sellers %>%
  mutate(seller_status = case_when(is.na(seller_status) & seller_id == "e9779976487b77c6d4ac45f75ec7afe9" & order_purchase_timestamp == "2017-03-01 09:25:31" ~ "new", 
                                   is.na(seller_status) & seller_id == "e9779976487b77c6d4ac45f75ec7afe9" & order_purchase_timestamp > "2017-03-01 09:25:31" ~ "old",
                                   TRUE ~ seller_status))

# Check for unusual or unique values in the seller_id column
unique_seller_ids <- unique(rev_from_sellers$seller_id)

na_seller_status <- sum(is.na(rev_from_sellers$seller_status))

cat("Unique seller_ids:", length(unique_seller_ids), "\n")
cat("NAs in seller_status:", na_seller_status, "\n")

# Remove the one remaining row where seller status is NA
rev_from_sellers <- rev_from_sellers %>%
  filter(!is.na(seller_status))

```

```{r}
library(ggplot2)

# Step 1: Count the number of unique orders for each seller
sales_per_seller <- rev_from_sellers %>%
  group_by(seller_id) %>%
  summarise(total_sales = n_distinct(order_id)) %>%
  ungroup()

# Step 2: Create a histogram
ggplot(sales_per_seller, aes(x = total_sales)) +
  geom_histogram(binwidth = 5, color = "black", fill = "blue") +
  labs(title = "Histogram of Sellers by Number of Total Sales",
       x = "Number of Sales",
       y = "Number of Sellers") +
  theme_minimal()

```

```{r}
# Step 1: Count the number of unique orders for each seller
sales_per_seller <- rev_from_sellers %>%
  group_by(seller_id) %>%
  summarise(total_sales = n_distinct(order_id)) %>%
  ungroup()

# Step 2: Create bins with a width of 2 for total sales and calculate the count of sellers in each bin
sales_bins <- cut(sales_per_seller$total_sales, breaks = seq(from = min(sales_per_seller$total_sales), to = max(sales_per_seller$total_sales), by = 2), right = FALSE, include.lowest = TRUE)
table_sales_bins <- table(sales_bins)

# Step 3: Calculate percent of total sellers for each bin
percent_sellers <- prop.table(table_sales_bins) * 100

# Step 4: Create a table displaying these percentages
percent_sellers_df <- data.frame('Sales Range' = names(percent_sellers), 'Percent of Total Sellers' = percent_sellers)

# View the table
percent_sellers_df

```
```{r}
# Assuming rev_from_sellers already exists and is prepared

library(lubridate)

# Step 1: Calculate total sales and active selling days per seller
seller_stats <- rev_from_sellers %>%
  group_by(seller_id) %>%
  summarise(first_sale = min(order_purchase_timestamp),
            last_sale = max(order_purchase_timestamp),
            total_sales = n_distinct(order_id)) %>%
  mutate(active_days = as.numeric(difftime(last_sale, first_sale, units = "days")) + 1, # Adding 1 to include the first day
         avg_sales_per_day = ifelse(active_days > 0, total_sales / active_days, 0)) %>%
  ungroup()

# Step 3: Visual Analysis
ggplot(seller_stats, aes(x = total_sales, y = avg_sales_per_day)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "blue", se = FALSE) +
  labs(title = "Average Sales per Day vs. Total Sales",
       x = "Total Sales",
       y = "Average Sales per Day") +
  theme_minimal()

# This plot should help identify the leveling point for avg_sales_per_day

```

```{r}
library(ggplot2)
library(lubridate)

# Step 1: Calculate total sales value and active selling days per seller
seller_value_stats <- rev_from_sellers %>%
  group_by(seller_id) %>%
  summarise(first_sale = min(order_purchase_timestamp),
            last_sale = max(order_purchase_timestamp),
            total_sales_value = sum(price)) %>%
  mutate(active_days = as.numeric(difftime(last_sale, first_sale, units = "days")) + 1, # Adding 1 to include the first day
         avg_sales_value_per_day = ifelse(active_days > 0, total_sales_value / active_days, 0)) %>%
  ungroup()

# Step 4: Plot the Data
ggplot(seller_value_stats, aes(x = total_sales_value, y = avg_sales_value_per_day)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "blue", se = FALSE) +
  labs(title = "Average Sales Value per Day vs. Total Sales Value",
       x = "Total Sales Value",
       y = "Average Sales Value per Day") +
  theme_minimal()

# This plot will help you visualize the relationship and identify any leveling off points.

```

```{r}
# Assuming rev_from_sellers already exists and is prepared

# Step 1 & 2: Calculate total number of orders and total sales value per seller
seller_orders_stats <- rev_from_sellers %>%
  group_by(seller_id) %>%
  summarise(total_orders = n_distinct(order_id),
            total_sales_value = sum(price),
            first_sale = min(order_purchase_timestamp),
            last_sale = max(order_purchase_timestamp)) %>%
  mutate(active_days = as.numeric(difftime(last_sale, first_sale, units = "days")) + 1, # Adding 1 to include the first day
         avg_sales_value_per_day = ifelse(active_days > 0, total_sales_value / active_days, 0)) %>%
  ungroup()

# Step 5: Plot the Data
ggplot(seller_orders_stats, aes(x = total_orders, y = avg_sales_value_per_day)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "blue", se = FALSE) +
  labs(title = "Average Sales Value per Day vs. Total Number of Orders",
       x = "Total Number of Orders",
       y = "Average Sales Value per Day") +
  theme_minimal()

# This plot will help you visualize the relationship and identify any leveling off points.

```

```{r}
library(zoo)

# Assuming seller_orders_stats is already created

# Calculate moving averages and variances with a rolling window
# Adjust the window size to smooth over the appropriate number of data points
window_size <- 50  # Window size can be changed based on data density

seller_orders_stats <- seller_orders_stats %>%
  arrange(total_orders) %>%
  mutate(moving_avg = rollmean(avg_sales_value_per_day, k = window_size, fill = NA, align = "right"),
         moving_var = rollapply(avg_sales_value_per_day, width = window_size, FUN = var, fill = NA, align = "right"))

# Plot the data with moving averages and variances
ggplot(seller_orders_stats, aes(x = total_orders)) +
  geom_line(aes(y = moving_avg), color = "blue") +
  geom_line(aes(y = sqrt(moving_var)), color = "red") +
  labs(title = "Moving Average and Standard Deviation of Avg Sales Value per Day",
       x = "Total Number of Orders",
       y = "Value") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_y_continuous(sec.axis = sec_axis(~.^2, name = "Variance"))

# Plotting the data may reveal when the average sales value per day stabilizes

```
```{r}
# Adjust the plotting code to zoom in on the first 500 orders

ggplot(seller_orders_stats, aes(x = total_orders)) +
  geom_line(aes(y = moving_avg), color = "blue") +
  geom_line(aes(y = sqrt(moving_var)), color = "red") +
  labs(title = "Moving Average and Standard Deviation of Avg Sales Value per Day (Up to 500 Orders)",
       x = "Total Number of Orders (up to 500)",
       y = "Value") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_y_continuous(sec.axis = sec_axis(~.^2, name = "Variance")) +
  coord_cartesian(xlim = c(0, 50)) # Zoom in on the x-axis

# This plot will focus on the total number of orders up to 500 and may provide a better view of when the average stabilizes.

```

```{r}
library(ggplot2)
library(dplyr)
library(zoo)

# Assuming rev_from_sellers is already created

# Use seller_orders_stats to filter for sellers with more than 3 orders
filtered_sellers <- seller_orders_stats %>%
  filter(total_orders > 3)

# Define the bin width
binwidth <- 50

# Find the max value for the x-axis
max_value <- max(filtered_sellers$avg_sales_value_per_day)

# Create breaks and labels for the histogram
breaks <- seq(0, max_value, by = binwidth)
labels <- paste(head(breaks, -1), head(breaks, -1) + binwidth - 1, sep = "-")

# Plot the histogram with custom breaks and labels
ggplot(filtered_sellers, aes(x = avg_sales_value_per_day)) +
  geom_histogram(binwidth = binwidth, fill = "blue", color = "black") +
  scale_x_continuous(breaks = head(breaks, -1), labels = labels) + # Exclude the last break for labels
  labs(title = "Histogram of Sellers' Average Daily Sales Value (More than 3 Orders)",
       x = "Average Daily Sales Value Range",
       y = "Number of Sellers") +
  theme_minimal()

```

```{r}
# Calculate active SaaS subcribers

library(lubridate)

# Set subscription start and end dates at first and last day of the months of first and last sales

subscription_info <- rev_from_sellers %>%
  group_by(seller_id) %>%
  summarise(subscription_start = min(order_purchase_timestamp),
            subscription_end = max(order_purchase_timestamp)) %>%
  mutate(subscription_start = floor_date(subscription_start, "month"),
         subscription_end = ceiling_date(subscription_end, "month") - days(1)) %>%
  ungroup()

# Join subscription info back to rev_from_sellers
rev_from_sellers <- rev_from_sellers %>%
  left_join(subscription_info, by = "seller_id")

```

```{r}
# Calculate daily subscription revenue and aggregate to daily DF

# Convert subscription start and end to dates
rev_from_sellers <- rev_from_sellers %>%
  mutate(subscription_start = as.Date(subscription_start),
         subscription_end = as.Date(subscription_end))

# Create daily DF for seller subscription
seller_daily_sub <- rev_from_sellers %>%
  rowwise() %>%
  mutate(days = list(seq.Date(subscription_start, subscription_end, by = "day"))) %>%
  unnest(days) %>%
  mutate(daily_fee = 99 / days_in_month(days)) %>%
  select(seller_id, days, daily_fee)

# Make sellers subscription DF daily
seller_daily_sub <- seller_daily

# Check that there is only one order_purchase_timestamp per order_id
order_timestamp_check <- rev_from_sellers %>%
  group_by(order_id) %>%
  summarize(n_timestamps = n_distinct(order_purchase_timestamp))%>%
  filter(n_timestamps > 1)

# Since only one timestamp per order we can aggregate main DF by order_id
rev_from_sellers_byorder_delivered <- rev_from_sellers %>%
  filter(order_status == "delivered") %>%
  group_by(order_id) %>%
  summarise(
    total_sales = sum(price),
    total_freight_value = sum(freight_value),
    total_units_ordered = n_distinct(order_item_id),
    order_purchase_timestamp = first(order_purchase_timestamp),
    unique_sellers = n_distinct(seller_id),
    unique_products = n_distinct(product_id)) %>%
  mutate(olist_rev_share = total_sales * 0.2)

# Make aggregated DF daily
rev_from_sellers_daily <- rev_from_sellers_byorder_delivered %>%
  group_by(order_purchase_timestamp = floor_date(order_purchase_timestamp, "day")) %>%
  summarise(
    total_sales = sum(total_sales),
    total_freight_value = sum(total_freight_value),
    total_units_ordered = sum(total_units_ordered),
    unique_sellers = sum(unique_sellers),
    unique_products= sum(unique_products),
    olist_rev_share = sum(olist_rev_share)
  )

# Merge subscription DF with main daily DF
rev_from_sellers_daily <- rev_from_sellers_daily %>%
  left_join(seller_daily_sub, by = c("order_purchase_timestamp" = "days"))

```

